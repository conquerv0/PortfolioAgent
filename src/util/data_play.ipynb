{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader.data as web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas_datareader\n",
      "  Downloading pandas_datareader-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting lxml (from pandas_datareader)\n",
      "  Downloading lxml-5.3.1-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from pandas_datareader) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from pandas_datareader) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.23->pandas_datareader) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.23->pandas_datareader) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.23->pandas_datareader) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=0.23->pandas_datareader) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->pandas_datareader) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->pandas_datareader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->pandas_datareader) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->pandas_datareader) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\conqu\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas_datareader) (1.17.0)\n",
      "Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
      "Downloading lxml-5.3.1-cp313-cp313-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 16.9 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml, pandas_datareader\n",
      "Successfully installed lxml-5.3.1 pandas_datareader-0.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 149\u001b[0m\n\u001b[0;32m    146\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeekly fixed income data saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweekly_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 81\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Import configuration variables (assumes you have a config.py with your FRED API key)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))))\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m     84\u001b[0m     load_dotenv()\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "import yfinance as yf\n",
    "import logging\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define the function to download FRED data for fixed income macro indicators and yields\n",
    "def download_daily_fred_series(api_key, start_date=\"2020-01-01\", end_date=None):\n",
    "    fred_local = Fred(api_key=api_key)\n",
    "    series_dict = {\n",
    "        \"EFFR\": \"EFFR\",          # Effective Federal Funds Rate\n",
    "        \"Headline_PCE\": \"PCE\",   # Headline PCE Price Index\n",
    "        \"Core_PCE\": \"PCEPILFE\",  # Core PCE Price Index\n",
    "        \"3M_Yield\": \"DGS3MO\",    # 3-Month Treasury Constant Maturity Rate\n",
    "        \"6M_Yield\": \"DGS6MO\",    # 6-Month Treasury Constant Maturity Rate\n",
    "        \"1Y_Yield\": \"DGS1\",      # 1-Year Treasury Constant Maturity Rate\n",
    "        \"2Y_Yield\": \"DGS2\",      # 2-Year Treasury Constant Maturity Rate\n",
    "        \"5Y_Yield\": \"DGS5\",      # 5-Year Treasury Constant Maturity Rate\n",
    "        \"10Y_Yield\": \"DGS10\",    # 10-Year Treasury Constant Maturity Rate\n",
    "        'IRLTLT01EZM156N': 'EUR_T10Y',  # Euro Area 10-Year Rate\n",
    "        'IRLTLT01JPM156N': 'JPY_T10Y',  # Japan 10-Year Rate\n",
    "        'IRLTLT01GBM156N': 'GBP_T10Y'   # UK 10-Year Rate\n",
    "    }\n",
    "    \n",
    "    data_frames = {}\n",
    "    for label, series_id in series_dict.items():\n",
    "        try:\n",
    "            series_data = fred_local.get_series(series_id, observation_start=start_date, observation_end=end_date)\n",
    "            df_series = series_data.to_frame(name=label)\n",
    "            data_frames[label] = df_series\n",
    "            logger.info(f\"Downloaded {label} from FRED.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading {series_id} ({label}): {e}\")\n",
    "    \n",
    "    df = pd.concat(data_frames.values(), axis=1)\n",
    "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df = df.reindex(full_index)\n",
    "    df = df.fillna(method='ffill')\n",
    "    return df\n",
    "\n",
    "# Compute momentum factors for the yield series\n",
    "def get_yield_momentum(df):\n",
    "    momentum_data = df.copy()\n",
    "    yield_cols = [\"3M_Yield\", \"6M_Yield\", \"1Y_Yield\", \"2Y_Yield\", \"5Y_Yield\", \"10Y_Yield\", \"EUR_T10Y\", \"JPY_T10Y\", \"GBP_T10Y\"]\n",
    "    for col in yield_cols:\n",
    "        if col in df.columns:\n",
    "            series = df[col].ffill()\n",
    "            momentum_data[f'{col}_mom_1m'] = series.pct_change(periods=21)\n",
    "            momentum_data[f'{col}_mom_3m'] = series.pct_change(periods=63)\n",
    "            momentum_data[f'{col}_mom_12m'] = series.pct_change(periods=252)\n",
    "            logger.info(f\"Computed momentum for {col}.\")\n",
    "    return momentum_data\n",
    "\n",
    "# Download risk sentiment data (VIX and MOVE) using yfinance\n",
    "def get_risk_sentiment_data(start_date='2020-01-01', end_date='2025-03-31'):\n",
    "    indices = {\n",
    "        '^VIX': 'VIX',\n",
    "        '^MOVE': 'MOVE'\n",
    "    }\n",
    "    risk_data = pd.DataFrame()\n",
    "    for ticker, name in indices.items():\n",
    "        try:\n",
    "            logger.info(f\"Downloading {ticker} data...\")\n",
    "            data = yf.download(ticker, start=start_date, end=end_date)['Close']\n",
    "            if not data.empty:\n",
    "                risk_data[name] = data\n",
    "                logger.info(f\"Successfully downloaded {ticker}.\")\n",
    "            else:\n",
    "                logger.warning(f\"No data received for {ticker}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading {ticker}: {e}\")\n",
    "    return risk_data\n",
    "\n",
    "def main():\n",
    "    # Import configuration variables (assumes you have a config.py with your FRED API key)\n",
    "    import sys\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "    FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "\n",
    "    # Set date ranges\n",
    "    full_start_date = '2020-01-01'  # For momentum calculations\n",
    "    target_start_date = '2023-11-01'  # Target period start for predictions\n",
    "    end_date = '2025-03-31'\n",
    "    \n",
    "    logger.info(\"Starting fixed income data collection...\")\n",
    "    \n",
    "    # Download macro and yield data from FRED\n",
    "    logger.info(\"Downloading FRED series...\")\n",
    "    fred_data = download_daily_fred_series(FRED_API_KEY, start_date=full_start_date, end_date=end_date)\n",
    "    logger.info(f\"Downloaded FRED data with shape: {fred_data.shape}\")\n",
    "    \n",
    "    # Compute momentum for yield series\n",
    "    logger.info(\"Computing yield momentum factors...\")\n",
    "    fred_with_momentum = get_yield_momentum(fred_data)\n",
    "    logger.info(f\"Data with momentum factors shape: {fred_with_momentum.shape}\")\n",
    "    \n",
    "    # Get risk sentiment data (VIX and MOVE)\n",
    "    logger.info(\"Downloading risk sentiment data...\")\n",
    "    risk_data = get_risk_sentiment_data(start_date=full_start_date, end_date=end_date)\n",
    "    if risk_data.empty:\n",
    "        logger.warning(\"No risk sentiment data collected.\")\n",
    "    else:\n",
    "        logger.info(f\"Risk sentiment data shape: {risk_data.shape}\")\n",
    "    \n",
    "    # Combine all data\n",
    "    logger.info(\"Combining FRED and risk sentiment data...\")\n",
    "    combined_data = pd.concat([fred_with_momentum, risk_data], axis=1)\n",
    "    logger.info(f\"Combined daily data shape: {combined_data.shape}\")\n",
    "    \n",
    "    # Filter for target period\n",
    "    combined_data = combined_data[target_start_date:end_date]\n",
    "    logger.info(f\"Data filtered to target period: {combined_data.shape}\")\n",
    "    \n",
    "    # Drop rows with missing essential yield data\n",
    "    required_yields = [\"3M_Yield\", \"6M_Yield\", \"1Y_Yield\", \"2Y_Yield\", \"5Y_Yield\", \"10Y_Yield\"]\n",
    "    initial_rows = len(combined_data)\n",
    "    combined_data = combined_data.dropna(subset=required_yields)\n",
    "    logger.info(f\"Dropped {initial_rows - len(combined_data)} rows due to missing yield data. Remaining rows: {len(combined_data)}\")\n",
    "    \n",
    "    # Save daily data to CSV\n",
    "    daily_file = 'data/fi_combined_features_daily.csv'\n",
    "    os.makedirs(os.path.dirname(daily_file), exist_ok=True)\n",
    "    combined_data.to_csv(daily_file)\n",
    "    logger.info(f\"Daily fixed income data saved to '{daily_file}'\")\n",
    "    \n",
    "    # Create weekly data by resampling (using Friday's data)\n",
    "    logger.info(\"Resampling daily data to weekly frequency...\")\n",
    "    weekly_data = combined_data.resample('W-FRI').last()\n",
    "    \n",
    "    # Calculate weekly changes for yield series and risk sentiment\n",
    "    for col in required_yields:\n",
    "        weekly_data[f'{col}_weekly_change'] = weekly_data[col].pct_change()\n",
    "    for sentiment in [\"VIX\", \"MOVE\"]:\n",
    "        if sentiment in weekly_data.columns:\n",
    "            weekly_data[f'{sentiment}_weekly_change'] = weekly_data[sentiment].pct_change()\n",
    "    \n",
    "    weekly_file = 'data/fi_combined_features_weekly.csv'\n",
    "    weekly_data.to_csv(weekly_file)\n",
    "    logger.info(f\"Weekly fixed income data saved to '{weekly_file}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
